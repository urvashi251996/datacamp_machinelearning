In this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities.

A 2D binary classification dataset is already loaded into the environment as X and y

//code
As you probably noticed, smaller values of C lead to less confident predictions. 
That's because smaller C means more regularization, which in turn means smaller coefficients, which means raw model outputs closer to zero and, thus,
probabilities closer to 0.5 after the raw model output is squashed through the sigmoid function. That's quite a chain of events!

# Set the regularization strength
model = LogisticRegression(C=0.1)

# Fit and plot
model.fit(X,y)
plot_classifier(X,y,model,proba=True)

# Predict probabilities on training points
prob = model.predict_proba(X)
print("Maximum predicted probability", np.max(prob))
